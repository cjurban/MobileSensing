{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking through a multilayer perceptron's computation\n",
    "\n",
    "## Code to accompany *Deep learning methods for mobile sensing*\n",
    "\n",
    "#### Please contact Christopher J. Urban at cjurban@live.unc.edu with any questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll walk through how a multilayer perceptron (MLP; a kind of artificial neural network) processes input data.\n",
    "\n",
    "The notebook is written in Python, so basic familiarity with Python could make things easier to follow, although it's not necessary.\n",
    "\n",
    "First, we'll import some necessary packages. \"sklearn\" refers to __[scikit-learn](https://scikit-learn.org/stable/)__, which is a very useful Python package for many machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import neural_network\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll simulate some data according to the following equation (also described in the text):\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "        \\text{NUD} &\\sim \\text{Bernoulli}(0.5) \\\\\n",
    "        \\text{AUD} &\\sim \\text{Bernoulli}(0.4) \\\\\n",
    "        \\text{MDD} &\\sim \\text{Bernoulli}\\Big(f\\big(-2 + 4 \\cdot  I(\\text{NUD} = \\text{AUD})\\big)\\Big),\n",
    "    \\end{split}\n",
    "\\end{align}\n",
    "where $I(\\cdot)$ is the indicator function, NUD is nicotine use disorder, AUD is alcohol use disorder, and MDD is major depressive disorder. In words, folks in this population have a $50\\%$ and $40\\%$ chance of having NUD and AUD, respectively, while chances of having MDD are high if you have both or neither NUD and AUD but low if you have only one of NUD or AUD. In other words, having MDD is nonlinearly related to having NUD and AUD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "N = 5000\n",
    "nud = np.random.binomial(1, 0.5, [N])\n",
    "aud = np.random.binomial(1, 0.4, [N])\n",
    "mdd = np.random.binomial(1, 1/(1 + np.exp(2-4 * (aud == nud))), [N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After simulating the data, we'll split it into a training and test set, then fit our MLP on the training set and compute its accuracy on the test set. Notice that we set several random seeds to ensure that results are reproducible. \n",
    "\n",
    "Note: There is also a convergence warning, which tells us that the scikit-learn default convergence criterion (i.e., criterion that determines when to stop the iterative fitting procedure) wasn't met. Our MLP already has good accuracy, so this isn't really a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP accuracy =  0.8696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gateslab/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([np.expand_dims(aud, axis = 1), np.expand_dims(nud, axis = 1)], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, mdd, test_size = 0.25, random_state = 234)\n",
    "\n",
    "# Fit MLP\n",
    "mlp_clf = neural_network.MLPClassifier(hidden_layer_sizes = (4, 4),\n",
    "                                      random_state = 456)\n",
    "mlp_pred = mlp_clf.fit(X_train, y_train).predict(X_test)\n",
    "mlp_acc = accuracy_score(y_test, mlp_pred)\n",
    "print(\"MLP accuracy = \", mlp_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also fit a logistic regression classifier to compare its performance to the MLP's. Its accuracy is much lower because, unlike the MLP, it fails to model the nonlinear relationship between the predictors and the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy =  0.5664\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression\n",
    "lr_clf = linear_model.LogisticRegression(penalty = \"none\",\n",
    "                                         solver = \"newton-cg\")\n",
    "lr_pred = lr_clf.fit(X_train, y_train).predict(X_test)\n",
    "lr_acc = accuracy_score(y_test, lr_pred)\n",
    "print(\"Logistic regression accuracy = \", lr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we step through how the fitted MLP processes input data (see text for more details). Here, we extract the fitted regression weights and intercepts, then use them to manually compute the predicted probability of having MDD for a specific individual. All fitted parameters and intermediate outputs are printed out below. Fitted values may be slightly different from those reported in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer computations\n",
      "x  = [1 0]\n",
      "W1 = [[-0.   -0.  ]\n",
      "     [-0.    0.  ]\n",
      "     [ 1.25  1.25]\n",
      "     [ 1.25  1.27]]\n",
      "b1 = [[-0.64]\n",
      "     [-0.7 ]\n",
      "     [-1.26]\n",
      "     [ 0.  ]]\n",
      "a1 = [[-0.64]\n",
      "     [-0.7 ]\n",
      "     [-0.  ]\n",
      "     [ 1.25]]\n",
      "h1 = [[0.  ]\n",
      "     [0.  ]\n",
      "     [0.  ]\n",
      "     [1.25]]\n",
      "\n",
      "Second layer computations\n",
      "W2 = [[ 0.    0.   -0.    0.  ]\n",
      "     [-0.   -0.   -0.   -0.  ]\n",
      "     [ 0.    0.   -0.    0.  ]\n",
      "     [-0.    0.   -2.67  1.32]]\n",
      "b2 = [[-0.52]\n",
      "     [-0.63]\n",
      "     [-0.5 ]\n",
      "     [-0.  ]]\n",
      "a2 = [[-0.52]\n",
      "     [-0.63]\n",
      "     [-0.5 ]\n",
      "     [ 1.65]]\n",
      "h2 = [[0.  ]\n",
      "     [0.  ]\n",
      "     [0.  ]\n",
      "     [1.25]]\n",
      "\n",
      "Output computations\n",
      "w3    = [[ 0.  ]\n",
      "        [ 0.  ]\n",
      "        [-0.  ]\n",
      "        [-2.32]]\n",
      "b3    = [1.77]\n",
      "a3    = [-2.06]\n",
      "y_hat = [0.11]\n"
     ]
    }
   ],
   "source": [
    "# First layer computations\n",
    "W1 = mlp_clf.coefs_[0]\n",
    "b1 = mlp_clf.intercepts_[0]\n",
    "x = np.array([1, 0]) \n",
    "a1 = np.matmul(x, W1) + b1\n",
    "h1 = np.maximum(a1, 0) # ReLU activation\n",
    "\n",
    "# Second layer computations\n",
    "W2 = mlp_clf.coefs_[1]\n",
    "b2 = mlp_clf.intercepts_[1]\n",
    "a2 = np.matmul(h1, W2) + b2\n",
    "h2 = np.maximum(a2, 0) # ReLU activation\n",
    "\n",
    "# Output computations\n",
    "w3 = mlp_clf.coefs_[2]\n",
    "b3 = mlp_clf.intercepts_[2]\n",
    "a3 = np.matmul(h2, w3) + b3\n",
    "y_hat = 1 / (1 + np.exp(-a3)) # Sigmoid final activation\n",
    "\n",
    "print(\"First layer computations\")\n",
    "print(\"x  =\", np.array2string(x.transpose(), prefix = \"x  =\"))\n",
    "print(\"W1 =\", np.array2string(np.around(W1.transpose(), 2), prefix = \"W1 =\"))\n",
    "print(\"b1 =\", np.array2string(np.around(np.expand_dims(b1.transpose(), axis = 1), 2), prefix = \"b1 =\"))\n",
    "print(\"a1 =\", np.array2string(np.around(np.expand_dims(a1.transpose(), axis = 1), 2), prefix = \"a1 =\"))\n",
    "print(\"h1 =\", np.array2string(np.around(np.expand_dims(h1.transpose(), axis = 1), 2), prefix = \"h1 =\"))\n",
    "\n",
    "print(\"\\nSecond layer computations\")\n",
    "print(\"W2 =\", np.array2string(np.around(W2.transpose(), 2), prefix = \"W2 =\"))\n",
    "print(\"b2 =\", np.array2string(np.around(np.expand_dims(b2.transpose(), axis = 1), 2), prefix = \"b2 =\"))\n",
    "print(\"a2 =\", np.array2string(np.around(np.expand_dims(a2.transpose(), axis = 1), 2), prefix = \"a2 =\"))\n",
    "print(\"h2 =\", np.array2string(np.around(np.expand_dims(h1.transpose(), axis = 1), 2), prefix = \"h2 =\"))\n",
    "\n",
    "print(\"\\nOutput computations\")\n",
    "print(\"w3    =\", np.array2string(np.around(w3, 2), prefix = \"w3    =\"))\n",
    "print(\"b3    =\", np.array2string(np.around(b3, 2), prefix = \"b3    =\"))\n",
    "print(\"a3    =\", np.array2string(np.around(a3, 2), prefix = \"a3    =\"))\n",
    "print(\"y_hat =\", np.array2string(np.around(y_hat, 2), prefix = \"y_hat =\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
